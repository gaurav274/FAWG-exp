{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import ray\n",
    "import click\n",
    "import torch\n",
    "\n",
    "from srtml import serve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', palette='muted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 13:50:34,116\tINFO resource_spec.py:212 -- Starting Ray with 324.56 GiB memory available for workers and up to 0.09 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-12-07 13:50:34,500\tINFO services.py:1170 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "serve.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig\n",
    "import torch\n",
    "from transformers.modeling_bert import BertLayer, BertEmbeddings, BertPooler, BertPreTrainingHeads\n",
    "from torch.nn import LayerNorm as BertLayerNorm\n",
    "\n",
    "class Bert(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Bert, self).__init__()\n",
    "        self.embedding_layer = BertEmbeddings(config)\n",
    "        self.layers = []\n",
    "        for i in range(config.num_hidden_layers-12):\n",
    "            self.layers.append(BertLayer(config))\n",
    "\n",
    "        self.layers = torch.nn.ModuleList(self.layers)\n",
    "        self.pooling_layer = BertPooler(config)\n",
    "        self.pre_training_heads_layer = BertPreTrainingHeads(config)\n",
    "        self.config = config;\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0,\n",
    "                                       std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, torch.nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input0, input1):\n",
    "        out0 = input0\n",
    "        out1 = input1\n",
    "        out = self.embedding_layer(out0, out1)\n",
    "        for layer in self.layers:\n",
    "            out,  = layer(out)\n",
    "        # out2 = self.pooling_layer(out)\n",
    "        # out3 = self.pre_training_heads_layer(out, out2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Stage0(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Stage0, self).__init__()\n",
    "        self.embedding_layer = BertEmbeddings(config)\n",
    "        self.layers = []\n",
    "#         print(config.num_hidden_layers)\n",
    "        for i in range(config.num_hidden_layers // 12):\n",
    "            self.layers.append(BertLayer(config))\n",
    "        len(self.layers)\n",
    "        self.layers = torch.nn.ModuleList(self.layers)\n",
    "        self.config = config\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0,\n",
    "                                       std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, torch.nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input0, input1):\n",
    "        out0 = input0\n",
    "        out1 = input1\n",
    "        out = self.embedding_layer(out0, out1)\n",
    "        for layer in self.layers:\n",
    "            out,  = layer(out)\n",
    "        return out\n",
    "\n",
    "class Stage1(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Stage1, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(config.num_hidden_layers // 12):\n",
    "            self.layers.append(BertLayer(config))\n",
    "        len(self.layers)\n",
    "        self.layers = torch.nn.ModuleList(self.layers)\n",
    "#         self.pooling_layer = BertPooler(config)\n",
    "#         self.pre_training_heads_layer = BertPreTrainingHeads(config)\n",
    "        self.config = config\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0,\n",
    "                                       std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, torch.nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input0):\n",
    "        out = input0\n",
    "        for layer in self.layers:\n",
    "            out,  = layer(out)\n",
    "#         out2 = self.pooling_layer(out)\n",
    "#         out3 = self.pre_training_heads_layer(out, out2)\n",
    "        return out\n",
    "\n",
    "class Stage2(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Stage2, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(config.num_hidden_layers // 12):\n",
    "            self.layers.append(BertLayer(config))\n",
    "        len(self.layers)\n",
    "        self.layers = torch.nn.ModuleList(self.layers)\n",
    "#         self.pooling_layer = BertPooler(config)\n",
    "#         self.pre_training_heads_layer = BertPreTrainingHeads(config)\n",
    "        self.config = config\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0,\n",
    "                                       std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, torch.nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input0):\n",
    "        out = input0\n",
    "        for layer in self.layers:\n",
    "            out,  = layer(out)\n",
    "#         out2 = self.pooling_layer(out)\n",
    "#         out3 = self.pre_training_heads_layer(out, out2)\n",
    "        return out\n",
    "\n",
    "class Stage3(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Stage3, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(config.num_hidden_layers // 12):\n",
    "            self.layers.append(BertLayer(config))\n",
    "        len(self.layers)\n",
    "        self.layers = torch.nn.ModuleList(self.layers)\n",
    "        self.pooling_layer = BertPooler(config)\n",
    "        self.pre_training_heads_layer = BertPreTrainingHeads(config)\n",
    "        self.config = config\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0,\n",
    "                                       std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, torch.nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input0):\n",
    "        out = input0\n",
    "        for layer in self.layers:\n",
    "            out,  = layer(out)\n",
    "#         out2 = self.pooling_layer(out)\n",
    "#         out3 = self.pre_training_heads_layer(out, out2)\n",
    "        return out\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "bert_config = BertConfig.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Debugging'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "encoded = tokenizer(text=txt, add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "                         max_length = 64,  # maximum length of a sentence\n",
    "                         padding='max_length',  # Add [PAD]s\n",
    "                         return_attention_mask = True,  # Generate the attention mask\n",
    "                         return_tensors = 'pt')\n",
    "inp = torch.rand(64, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 1024])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 8\n",
    "gpu = 4\n",
    "inp1 = torch.stack([encoded['input_ids'][0]]*bs)\n",
    "inp2 = torch.stack([encoded['attention_mask'][0]]*bs)\n",
    "inp3 = torch.stack([inp]*bs)\n",
    "inp3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs, gpu in zip([1,2,4,8], [0,1,2,3]):\n",
    "    inp1 = torch.stack([encoded['input_ids'][0]]*bs)\n",
    "    inp2 = torch.stack([encoded['attention_mask'][0]]*bs)\n",
    "    inp3 = torch.stack([inp]*bs)\n",
    "\n",
    "    se = Stage0(bert_config).cuda(gpu)\n",
    "    se2 = Stage3(bert_config).cuda(gpu+4)\n",
    "    se(inp1.cuda(gpu), inp2.cuda(gpu))\n",
    "    se2(inp3.cuda(gpu+4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_type = 'torch'\n",
    "batch_size = 1\n",
    "def noop(*args, **kwargs):\n",
    "    bs = serve.context.batch_size\n",
    "    time.sleep(0.1)\n",
    "#     assert (\n",
    "#         bs == batch_size\n",
    "#     ), f\"worker received {batch_size} which is not what expected {batch_size}\"\n",
    "    result = \"\"\n",
    "    \n",
    "    if return_type == \"torch\":\n",
    "        result = torch.zeros((3, 224, 224))\n",
    "\n",
    "    if bs is None:  # No batching\n",
    "        return result\n",
    "    else:\n",
    "        return [result] * bs\n",
    "\n",
    "noop = serve.accept_batch(noop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = BertConfig.from_pretrained('bert-large-uncased')\n",
    "\n",
    "class BertClass:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model = self.model.cuda()\n",
    "    \n",
    "    def __call__(self, data: list) -> list:\n",
    "        input0 = torch.stack([item['input_ids'][0] for item in data])\n",
    "        input1 = torch.stack([item['attention_mask'][0] for item in data])\n",
    "\n",
    "        \n",
    "        input0 = input0.cuda()\n",
    "        input1 = input1.cuda()\n",
    "        outputs = self.model(input0, input1)\n",
    "        res = list(outputs.cpu().unbind())\n",
    "        return res\n",
    "\n",
    "def bert(*args, data, **kwargs):\n",
    "\n",
    "    model = Bert(bert_config)\n",
    "    model.cuda()\n",
    "    input0 = torch.stack([item['input_ids'][0] for item in data])\n",
    "    input1 = torch.stack([item['attention_mask'][0] for item in data])\n",
    "\n",
    "    input0 = input0.cuda()\n",
    "    input1 = input1.cuda()\n",
    "    outputs = model(input0, input1)\n",
    "    res = list(outputs.cpu().unbind())\n",
    "    return res\n",
    "\n",
    "\n",
    "def stage0(*args, data, **kwargs):\n",
    "\n",
    "    model = Stage0(bert_config)\n",
    "#     model.cuda()\n",
    "    input0 = torch.stack([item['input_ids'][0] for item in data])\n",
    "    input1 = torch.stack([item['attention_mask'][0] for item in data])\n",
    "\n",
    "    input0 = input0.cuda()\n",
    "    input1 = input1.cuda()\n",
    "    outputs = model(input0, input1)\n",
    "    res = list(outputs.cpu().unbind())\n",
    "    return res\n",
    "\n",
    "def stage1(*args, data, **kwargs):\n",
    "    model = Stage1(bert_config)\n",
    "    model.cuda()\n",
    "#     print('stage1: ', data[0].shape)\n",
    "    input0 = torch.stack(data)\n",
    "    input0 = input0.cuda()\n",
    "    outputs = model(input0)\n",
    "    res = list(outputs.cpu().unbind())\n",
    "    return res\n",
    "\n",
    "def stage2(*args, data, **kwargs):\n",
    "    model = Stage2(bert_config)\n",
    "    model.cuda()\n",
    "#     print('stage2: ', data[0].shape)\n",
    "    input0 = torch.stack(data)\n",
    "    input0 = input0.cuda()\n",
    "    outputs = model(input0)\n",
    "    res = list(outputs.cpu().unbind())\n",
    "    return res\n",
    "\n",
    "def stage3(*args, data, **kwargs):\n",
    "    model = Stage3(bert_config)\n",
    "    model.cuda()\n",
    "#     print('stage3: ', data[0].shape)\n",
    "    input0 = torch.stack(data)\n",
    "    input0 = input0.cuda()\n",
    "    outputs = model(input0)\n",
    "    res = list(outputs.cpu().unbind())\n",
    "    return res\n",
    "\n",
    "Bert = serve.accept_batch(Bert)\n",
    "bert = serve.accept_batch(bert)\n",
    "stage3 = serve.accept_batch(stage3)\n",
    "stage2 = serve.accept_batch(stage2)\n",
    "stage1 = serve.accept_batch(stage1)\n",
    "stage0 = serve.accept_batch(stage0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with serve.using_router(\"noop\"):\n",
    "    serve.create_endpoint(\"noop\", \"/noop\")\n",
    "    config = serve.BackendConfig(max_batch_size=batch_size)\n",
    "#     config = serve.BackendConfig()\n",
    "    serve.create_backend(noop, \"noop\", backend_config=config)\n",
    "    serve.link(\"noop\", \"noop\")\n",
    "    handle = serve.get_handle(\"noop\")\n",
    "\n",
    "num_backends = 1\n",
    "backend = [Bert]#[stage0, stage1, stage2, stage3]\n",
    "handle = [None]*num_backends\n",
    "\n",
    "for i in range(num_backends):\n",
    "    router_name = 'stage' + str(i)\n",
    "    with serve.using_router(router_name):\n",
    "        serve.create_endpoint(router_name, '/'+router_name)\n",
    "        config = serve.BackendConfig(max_batch_size=batch_size, num_gpus=1)#, resources={'Tesla P40':1})\n",
    "    #     config = serve.BackendConfig()\n",
    "        serve.create_backend(backend[i], router_name, backend_config=config)\n",
    "        serve.link(router_name, router_name)\n",
    "        handle[i] = serve.get_handle(router_name)\n",
    "\n",
    "# with serve.using_router(\"noop2\"):\n",
    "#     serve.create_endpoint(\"noop2\", \"/noop2\")\n",
    "#     config = serve.BackendConfig(max_batch_size=batch_size, num_gpus=1)\n",
    "# #     config = serve.BackendConfig()\n",
    "#     serve.create_backend(stage1, \"noop2\", backend_config=config)\n",
    "#     serve.link(\"noop2\", \"noop2\")\n",
    "#     handle2 = serve.get_handle(\"noop2\")\n",
    "\n",
    "num_queries = 30\n",
    "num_warmups = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "[[tensor([[ 0.8785, -0.6656, -0.9790,  ..., -0.3431,  0.5983,  0.7632],\n",
      "        [ 0.0077,  0.0257, -2.0476,  ..., -0.6537,  0.9058,  0.6314],\n",
      "        [ 0.9455, -0.5244, -1.6135,  ...,  0.4624,  0.7413,  0.0031],\n",
      "        ...,\n",
      "        [ 0.8474, -0.1775, -1.9153,  ..., -0.8308, -0.5692,  0.7796],\n",
      "        [ 0.1397, -0.7879, -1.0723,  ..., -1.4896,  0.3921,  0.1265],\n",
      "        [ 0.9242, -0.0896, -0.6089,  ..., -0.8795, -0.9089,  0.3433]],\n",
      "       requires_grad=True)]]\n",
      "[[tensor([[ 2.2249, -0.2349,  0.0603,  ..., -0.5012, -0.2063,  0.8447],\n",
      "        [ 1.1882, -0.8061, -0.9110,  ...,  0.1674, -0.9667,  1.0657],\n",
      "        [ 1.1026,  0.1346, -0.4158,  ...,  0.2178, -0.2201,  1.0791],\n",
      "        ...,\n",
      "        [ 1.4115, -1.6898,  0.4669,  ..., -0.3013, -1.0541, -0.0807],\n",
      "        [ 1.6818, -0.6471, -0.0762,  ...,  0.1737,  0.1678, -0.2517],\n",
      "        [ 2.2949, -1.0060, -0.6091,  ..., -0.5323, -0.9178,  0.8164]],\n",
      "       requires_grad=True)]]\n",
      "[[tensor([[ 0.2354, -0.4642,  0.0224,  ..., -0.6754, -0.6071, -1.0677],\n",
      "        [ 1.7160, -0.0106,  1.0062,  ..., -0.7573,  0.4187, -0.3545],\n",
      "        [ 0.0417, -1.6520,  0.3914,  ...,  0.0297, -0.3866, -0.6865],\n",
      "        ...,\n",
      "        [-0.6707, -0.6476,  0.7752,  ..., -0.1251,  0.1322,  0.1160],\n",
      "        [ 0.1076, -0.4258,  0.6880,  ..., -0.1995,  1.0482, -0.3283],\n",
      "        [-0.4746, -1.2050,  0.1792,  ...,  0.0730,  0.3841,  1.0312]],\n",
      "       requires_grad=True)]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9f01d42df6f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# x = ray.wait([ppu_handle.enqueue_batch(data=[encoded])], num_returns=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menqueue_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/srtml-exp/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_ids, timeout)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mlast_task_error_raise_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/srtml-exp/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_ids, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0;32m--> 306\u001b[0;31m             object_ids, self.current_task_id, timeout_ms)\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_metadata_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latency = []\n",
    "txt = 'Debugging'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "encoded = tokenizer(text=txt, add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "                         max_length = 64,  # maximum length of a sentence\n",
    "                         padding='max_length',  # Add [PAD]s\n",
    "                         return_attention_mask = True,  # Generate the attention mask\n",
    "                         return_tensors = 'pt')\n",
    "print(encoded['input_ids'].shape)\n",
    "prepoc_dummy_kwarg = {\"data\": [txt]}\n",
    "\n",
    "latency_list_ms = []\n",
    "for _ in range(10):\n",
    "    start_time = time.time()\n",
    "    # x = ray.wait([ppu_handle.enqueue_batch(data=[encoded])], num_returns=1)\n",
    "    x = ray.get([handle[0].enqueue_batch(data=[encoded])])\n",
    "    print(x)\n",
    "    end_time = time.time()\n",
    "    time_taken_ms = (end_time - start_time) * 1000\n",
    "    latency_list_ms.append(time_taken_ms)\n",
    "pprint(latency_list_ms)\n",
    "\n",
    "for i in tqdm(range(num_warmups + num_queries)):\n",
    "    if i == num_warmups:\n",
    "        serve.clear_trace()\n",
    "    oids = []\n",
    "    start = time.perf_counter()\n",
    "    x = None\n",
    "    if not batch_size:\n",
    "        ray.get(\n",
    "            # This is how to pass a higher level metadata to the tracing\n",
    "            # context\n",
    "            handle.options(\n",
    "                tracing_metadata={\"pipeline-id\": i}\n",
    "            ).remote()\n",
    "        )\n",
    "    else:\n",
    "#         r = handle1.options(tracing_metadata={\"pipeline-id\": i}).remote(\n",
    "#                 data=encoded\n",
    "#             )\n",
    "#         r = handle2.options(tracing_metadata={\"pipeline-id\": i}).remote(\n",
    "#                  data=r  # torch tensor\n",
    "#             )\n",
    "#         oids.append(r)\n",
    "\n",
    "#         x = ray.get(handle1.enqueue_batch(val=ray.get(handle.enqueue_batch(val=[1] * batch_size))))\n",
    "        #x = ray.get(handle2.options(tracing_metadata={\"pipeline-id\": i}).remote(data=ray.get(handle1.options(tracing_metadata={\"pipeline-id\": i}).remote(data = encoded))))\n",
    "#         ray.get([handle.remote() for _ in range(batch_size)])\n",
    "#         print(x)\n",
    "        x = ray.get([handle[0].enqueue_batch(data=[encoded])])\n",
    "#         x1 = ray.get(handle[0].options(tracing_metadata={'pipeline-id':i}).remote(data=encoded))\n",
    "#         x2 = ray.get(handle[1].options(tracing_metadata={'pipeline-id':i}).remote(data=ray.get(handle[0].options(tracing_metadata={'pipeline-id':i}).remote(data=encoded))))\n",
    "#         x3 = ray.get(handle[2].options(tracing_metadata={'pipeline-id':i}).remote(data=ray.get(handle[1].options(tracing_metadata={'pipeline-id':i}).remote(data=ray.get(handle[0].options(tracing_metadata={'pipeline-id':i}).remote(data=encoded))))))\n",
    "#         x4 = ray.get(handle[1].options(tracing_metadata={\"pipeline-id\": i}).remote(data=ray.get(handle[0].options(tracing_metadata={\"pipeline-id\": i}).remote(data = encoded))))\n",
    "#         x4 = ray.get(handle[3].options(tracing_metadata={'pipeline-id':i}).remote(data=ray.get(handle[2].options(tracing_metadata={'pipeline-id':i}).remote(data=ray.get(handle[1].options(tracing_metadata={'pipeline-id':i}).remote(data=ray.get(handle[0].options(tracing_metadata={'pipeline-id':i}).remote(data=encoded))))))))\n",
    "#     x = ray.wait(oids, len(oids))\n",
    "    end = time.perf_counter()\n",
    "    latency.append(end - start)\n",
    "    \n",
    "# Remove initial samples\n",
    "latency = latency[num_warmups:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6479.653358459473,\n",
       " 2719.071388244629,\n",
       " 2703.2644748687744,\n",
       " 2689.974069595337,\n",
       " 2713.3231163024902,\n",
       " 2717.5133228302,\n",
       " 2701.0676860809326,\n",
       " 2712.5818729400635,\n",
       " 2882.8675746917725,\n",
       " 2726.3646125793457]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_list_ms = []\n",
    "for _ in range(10):\n",
    "    start_time = time.time()\n",
    "    ray.wait([handle[0].enqueue_batch(data=[encoded])], num_returns=1)\n",
    "    end_time = time.time()\n",
    "    time_taken_ms = (end_time - start_time) * 1000\n",
    "    latency_list_ms.append(time_taken_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_list_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "series = pd.Series(latency) * 1000\n",
    "print(\"Latency for single noop backend (ms)\")\n",
    "print(series.describe(percentiles=[0.5, 0.9, 0.95, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_file = 'trace.json'\n",
    "with open(trace_file, \"w\") as f:\n",
    "    json.dump(serve.get_trace(), f)\n",
    "print(f\"trace file written to {trace_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = serve.get_trace()\n",
    "df = pd.DataFrame(data['metadata'])\n",
    "df = df.transpose()\n",
    "print(df)\n",
    "df = pd.DataFrame(data['traces'])\n",
    "df = df.sort_values(['query_id', 'event', 'time_us']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['traces'])\n",
    "df = df.sort_values(['query_id', 'event', 'time_us']).reset_index(drop=True)\n",
    "df1 = pd.DataFrame()\n",
    "c = 0\n",
    "prev = None\n",
    "for a,q,i in df.itertuples(index=False):\n",
    "#     print(q)\n",
    "    if i == prev:\n",
    "        c+=1\n",
    "        df1 = df1.append({'query_id': str(q), 'event': i+'_'+str(c)}, ignore_index = True)\n",
    "    else:\n",
    "        c = 0\n",
    "        df1 = df1.append({'query_id': str(q), 'event': i+'_'+str(c)}, ignore_index = True)\n",
    "        prev = i\n",
    "# df.loc[df['query_id'] == 514]\n",
    "# df.head(20)\n",
    "df1.head(20)\n",
    "df[['event', 'query_id']] = df1\n",
    "df.head(50)\n",
    "# df = df.sort_values('time_us')\n",
    "# df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pivot(index='query_id', columns=\"event\").droplevel(0,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ordering = [\n",
    "    'router_enqueue',\n",
    "    \"router_dequeue\",\n",
    "    \"worker_start\",\n",
    "    \"worker_done\",\n",
    "    \"router_recv_result\"\n",
    "]\n",
    "new_event_ordering = []\n",
    "for i in range(num_backends):\n",
    "    for event in event_ordering:\n",
    "          new_event_ordering.append(event+ '_' + str(i)) \n",
    "event_ordering = new_event_ordering\n",
    "series = {}\n",
    "for e_start, e_end in zip(event_ordering, event_ordering[1:]):\n",
    "    series[f\"{e_start}->{e_end}\"] = (df[e_end] - df[e_start])/1000\n",
    "diff_df = pd.DataFrame(series)\n",
    "\n",
    "diff_df.to_csv('data.csv')\n",
    "print(diff_df.mean(axis=0))\n",
    "# diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, s in diff_df.T.iterrows():\n",
    "    sns.distplot(s, label=name, kde_kws={\"cumulative\": True})\n",
    "plt.legend()\n",
    "plt.xlabel(\"us\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
